# LLM Provider Configuration
# Uncomment and configure the provider you want to use

# OpenAI
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini

# Anthropic Claude
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Google Gemini
GOOGLE_API_KEY=your_google_api_key_here
GEMINI_MODEL=gemini-2.0-flash-exp

# Default LLM Provider (openai, anthropic, gemini)
DEFAULT_LLM_PROVIDER=openai

# Embedding Configuration
# Model for semantic search (default: paraphrase-multilingual-mpnet-base-v2)
EMBEDDING_MODEL=paraphrase-multilingual-mpnet-base-v2
EMBEDDING_DEVICE=cpu  # or cuda for GPU

# Database Configuration
DATABASE_PATH=data/legal_docs.db
LLM_CACHE_PATH=data/llm_cache.db

# Knowledge Graph Paths
KG_PATH=data/kg_enhanced/legal_kg.json
CHAPTER_SUMMARIES_PATH=data/kg_enhanced/chapter_summaries.json
ARTICLE_SUMMARIES_PATH=data/kg_enhanced/article_summaries.json

# Retrieval Configuration
# Number of articles to retrieve
TOP_K=10

# DualLevel Retriever Weights (6-component scoring)
WEIGHT_KEYPHRASE=0.15
WEIGHT_SEMANTIC=0.25
WEIGHT_PPR=0.20
WEIGHT_CONCEPT=0.15
WEIGHT_THEME=0.15
WEIGHT_HIERARCHY=0.10

# RRF Configuration (Semantic Bridge)
RRF_K=60  # Reciprocal Rank Fusion constant
AGREEMENT_BONUS=0.2  # Bonus for articles appearing in both tiers

# Tree Traversal Configuration
TREE_MAX_CHAPTERS=3  # Max chapters to explore in Loop 1
TREE_MAX_ARTICLES=5  # Max articles to retrieve per chapter in Loop 2

# PPR Configuration
PPR_ALPHA=0.15  # Damping factor (0.15 = 15% teleport probability)
PPR_MAX_ITER=100
PPR_TOL=1e-6

# Logging
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR
LOG_FILE=logs/vn_legal_rag.log

# Performance
ENABLE_CACHE=true
CACHE_TTL=86400  # Cache time-to-live in seconds (24 hours)
MAX_WORKERS=4  # Parallel workers for batch processing
